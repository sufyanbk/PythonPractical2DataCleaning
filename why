| **DSE Activity**                                                   | **Challenge / Symptom**                                                                                                                                                 | **Impact**                                                                                              | **Current Workaround**                                                                                     | **Desired Outcome (Solutionised)**                                                                                                                                                      |
| ------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Dataset generation (via master plans, custom feature plans)**    | Jobs take hours → days; runtime highly variable; unpredictable; plans over large datasets can take weeks; crash with poor error feedback                                | Sprint delays (e.g. FE6/FE8 blocked for weeks); months of work stalled; experimentation discouraged     | Run overnight or weekends; wait for jobs to finish; abort jobs (if permissions allow); rely on IT for logs | Pre-flight ETA + cost model (predict runtime/resources); auto-queuing (e.g. schedule for 7pm); checkpointing to avoid re-reading data; user-friendly error messages with stage/location |
| **Feature coding & validation (plans on years of data)**           | Extremely slow on large datasets; jobs sometimes fail without actionable errors; lack of clarity on restrictions (hidden rules, null strings, etc.)                     | 3–4 weeks lost on feature creation; blocked downstream model builds; onboarding confusion for new users | Trial-and-error debugging; escalate to seniors; request IT logs                                            | Descriptive error messages; access to logs; documentation of platform restrictions; job templates for standard feature coding                                                           |
| **Model training**                                                 | Training setup fine, but waiting for compute resources slows jobs; bottlenecks when cores are busy                                                                      | Training blocked during heavy DSC use; time wasted idling                                               | Wait until DSC less busy; run late-night                                                                   | Resource partitioning to guarantee baseline compute; priority queueing for sprint-critical training jobs                                                                                |
| **Model evaluations**                                              | Multiple models need evaluation → queued sequentially or in parallel; system slows drastically if many jobs run; progress bar misleading (hangs at 99%); cryptic errors | Half-days lost; sprint-critical deadlines (e.g. Friday 3pm) overrun; evaluations crash without clarity  | Queue evaluations one by one; limit models manually; wait for jobs to complete                             | Job classification (L/H) → lightweight evals routed to small pool; ETA displayed per evaluation; better error context; ability to queue/schedule jobs safely                            |
| **JupyterLab (analysis, PySpark, KPI generation, ad-hoc queries)** | Becomes unusable when heavy jobs run; simple tasks (file read, small PySpark job) take minutes; crashes during DSC traffic; tied to DSC cores                           | Analysts blocked from exploratory analysis; discourages experimentation; KPI generation delayed         | Export data to DSW/BigQuery for compute (slower, not intended use); run after-hours                        | Resource partitioning → Interactive pool (15–30%) always reserved; decouple JupyterLab baseline from DSC heavy cores; hot-dataset caching for ad-hoc queries                            |
| **Debugging & error handling**                                     | Error messages vague (“core crash”); logs inaccessible to most teams; progress bar not representative; hidden platform rules not documented                             | Days wasted identifying cause; heavy reliance on IT; steep learning curve for juniors                   | Escalate to IT; ask colleagues; guess and re-run                                                           | Standardised log access across markets; user-friendly error messages (“failed at metric X”); inline hints; post-run performance reports with optimisation suggestions                   |
| **Job scheduling & resource management**                           | No visibility into queue status; users can’t see who is running heavy jobs; lack of fair-share → one job monopolises cores                                              | Sprint planning impossible; wasted compute; small jobs blocked; unfair utilisation                      | Informal pinging colleagues; wait blindly; abort jobs if permissions allow                                 | Live Ops panel (cluster load, active jobs, queue status, ETA); fair-share scheduler; resource pools (interactive vs heavy); job classification with auto-routing                        |
| **Overall system stability**                                       | DSC outage can last days (e.g. full week downtime); platform crash if one job fails catastrophically; lack of fail-safes                                                | Sprints blocked; 100+ hours lost per sprint; high-risk single points of failure                         | IT escalation; pause work until resolved                                                                   | Safety-net architecture: failover rules so one job can’t crash whole platform; exception handling at job level                                                                          |
