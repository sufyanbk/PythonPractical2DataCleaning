DSE Activity,Challenges / Symptoms,Impact (to team/sprint),Current Workarounds,Desired Outcomes (solutionised)
"Dataset Generation (Master Plans, Custom Plans)",Jobs take hours → days; sometimes stretch to weeks; failures with cryptic error messages; progress bar misleading (hangs at 99%); extracting datasets is extremely slow; crashes cause re-runs from scratch,Sprint delays (3–4 weeks lost on feature plans); wasted compute & time on re-runs; frequent DSE users/data scientists blocked from timely data pulls,Run overnight or weekends; abort and restart jobs; escalate to IT for logs; export subsets to other platforms,Pre-flight ETA (P50/P90); auto-queuing for off-peak; checkpoint & auto-retry; transparent error messages that point to cause/fix
Feature Coding & Validation,Running features over long histories (years) takes days/weeks; failures without explanations; restrictions undocumented (hidden rules),Blocked downstream models; onboarding difficult for inexperienced users; 3–4 weeks lost on some features,Trial-and-error debugging; heavy reliance on experienced colleagues & IT; guessing fixes,Clear documentation of platform rules; user-friendly error messages (‘failed at metric X’); log access to all teams; job templates for standard feature runs
Model Training,Slowed when cluster busy; competes with evaluations and plans; runtime unpredictable,Delays in experiments; reduced velocity in sprint cycles,Schedule jobs out-of-hours; limit parallel runs manually,Resource partitioning (training vs evals); priority queueing for sprint-critical training jobs
Model Evaluations,Multiple evaluations queued → long waits; runtime unpredictable; failures after hours wasted; progress bar unreliable; hard to track if all evaluations completed successfully,Sprint deadlines missed; repeated re-runs; confidence in results low,Queue manually; monitor logs manually; abort when needed,ETA forecasting with P50/P90; ability to auto-queue evaluations; checkpoint & auto-resume; more granular progress indicators
"JupyterLab (Analysis, PySpark, KPI/Ad-hoc)","Freezes/crashes if heavy jobs running; simple tasks (file read, PySpark) take minutes; KPI generation & small dataset queries blocked during heavy load",Daily work stalls; frequent DSE users/data scientists can’t explore ad-hoc; morale and efficiency reduced; work shifted outside core hours,Export data to DSW/BigQuery; run analysis late evenings,Resource partitioning: dedicated Interactive pool (15–30%); guaranteed baseline compute for small tasks; hot dataset caching for fast access
Debugging & Error Handling,Cryptic error messages (‘core crash’); logs restricted (only some regions have access); progress bars misleading; difficult to pinpoint error source,Wasted days debugging; heavy reliance on IT; slows onboarding & productivity for inexperienced users,Escalate to IT; support from experienced colleagues; guess and retry jobs,Consistent log access across markets; actionable error messages; inline hints at failure point; post-run performance reports with optimisations
Job Scheduling & Resource Management,Users can’t see queue or who’s hogging resources; one job can monopolise cluster if protections off; no ability to forecast queue times; waits unpredictable,People abort others’ jobs (wasted compute); sprint planning impossible; productivity highly variable,Ping colleagues informally; wait blindly; abort jobs if permissions allow,"Live Ops panel (queue status, active jobs, ETA); fair-share scheduler (DRF); executor ceilings & user concurrency caps"
System Stability,Occasional platform-wide outages; one failure can cascade to crash cluster,Multi-day productivity loss; high dependency risk,Escalate to IT; pause work,Safety-net architecture (job-level isolation); failover rules to prevent cluster-wide impact
